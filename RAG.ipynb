{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOd77ySgHyCmkZ79EXIYY+6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kksona/RAG-QnA/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXRHB3w7RBfL",
        "outputId": "4e8ec94e-d1e9-4b81-d6ca-f32fc0327f03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.0/19.0 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.9/194.9 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "! pip install -q --upgrade google-generativeai langchain-google-genai chromadb pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "import textwrap\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "FzyWOqWPSiX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "import os\n",
        "GEMINI_API_KEY=userdata.get('GEMINI_API_KEY')\n",
        "genai.configure(api_key=GEMINI_API_KEY)"
      ],
      "metadata": {
        "id": "aFVkOzrVSoiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LangChain connected to Google Gemini"
      ],
      "metadata": {
        "id": "382kI-NhUX7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\",google_api_key=GEMINI_API_KEY)"
      ],
      "metadata": {
        "id": "iVW49xbEUXQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm.invoke(\"What are the usecases of System Design?\")"
      ],
      "metadata": {
        "id": "29X81Nh3Uua7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_markdown(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sChcEa2JVMNz",
        "outputId": "14ccce0e-254f-44c2-e8d4-3d0fe4205d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> System design is the process of defining the architecture, modules, interfaces, and data for a system to satisfy specified requirements. Its applications are vast and span across numerous industries. Here's a breakdown of common use cases:\n> \n> **1. Building Scalable and Reliable Web Applications:**\n> \n> *   **E-commerce Platforms (Amazon, Shopify):** Handling millions of users, products, and transactions requires robust system design to ensure high availability, scalability, and fault tolerance.  This includes considerations for database sharding, caching, load balancing, and microservices.\n> *   **Social Media Platforms (Facebook, Twitter):**  Managing massive amounts of data (posts, images, videos, user relationships), real-time feeds, and personalized content delivery demands a well-designed distributed system.  Key aspects include content delivery networks (CDNs), graph databases, and message queues.\n> *   **Streaming Services (Netflix, Spotify):** Delivering high-quality video and audio content to millions of users concurrently requires efficient content encoding, storage, and delivery mechanisms.  This involves considerations for adaptive bitrate streaming, content distribution networks, and content protection.\n> *   **Search Engines (Google, Bing):** Indexing and searching vast amounts of web data in milliseconds requires sophisticated indexing algorithms, distributed search infrastructure, and ranking algorithms.\n> \n> **2. Developing Mobile Applications:**\n> \n> *   **Ride-Hailing Apps (Uber, Lyft):**  Matching drivers and riders in real-time, handling location data, and processing payments requires a robust backend system.  This involves considerations for geolocation databases, real-time communication systems, and payment gateway integrations.\n> *   **Messaging Apps (WhatsApp, Telegram):**  Delivering messages reliably and securely to millions of users requires a distributed messaging system.  Key aspects include message queues, encryption, and push notification systems.\n> *   **Gaming Apps:**  Handling real-time multiplayer interactions, game state synchronization, and user authentication requires a well-designed backend system.\n> \n> **3. Designing Data-Intensive Applications:**\n> \n> *   **Data Warehouses:**  Storing and analyzing large volumes of historical data for business intelligence.  This involves considerations for data modeling, ETL (Extract, Transform, Load) processes, and analytical query optimization.\n> *   **Big Data Analytics Platforms:**  Processing and analyzing large datasets using frameworks like Hadoop, Spark, and Flink.  This involves considerations for data partitioning, parallel processing, and fault tolerance.\n> *   **Real-time Analytics Systems:**  Analyzing streaming data in real-time to detect anomalies, trends, and patterns.  This involves considerations for stream processing engines, message queues, and real-time databases.\n> *   **Machine Learning Platforms:**  Training and deploying machine learning models at scale.  This involves considerations for distributed training, model serving, and model monitoring.\n> \n> **4. Building Distributed Systems:**\n> \n> *   **Cloud Computing Platforms (AWS, Azure, GCP):**  Providing on-demand computing resources, storage, and services requires a highly scalable and reliable distributed system.  This involves considerations for virtualization, containerization, and distributed resource management.\n> *   **Blockchain Technology:**  Implementing decentralized and secure ledgers for various applications.  This involves considerations for consensus algorithms, cryptography, and peer-to-peer networking.\n> *   **Internet of Things (IoT) Platforms:**  Collecting and processing data from a large number of connected devices.  This involves considerations for message brokers, data storage, and device management.\n> \n> **5. Improving Existing Systems:**\n> \n> *   **Performance Optimization:** Identifying and addressing performance bottlenecks in existing systems to improve response times and throughput.  This involves considerations for caching, database optimization, and code profiling.\n> *   **Scalability Enhancement:**  Modifying existing systems to handle increasing user load and data volume.  This involves considerations for load balancing, database sharding, and microservice architecture.\n> *   **Reliability Improvement:**  Adding redundancy and fault tolerance to existing systems to minimize downtime.  This involves considerations for replication, failover mechanisms, and monitoring.\n> *   **Security Enhancement:**  Implementing security measures to protect existing systems from unauthorized access and attacks.  This involves considerations for authentication, authorization, and encryption.\n> \n> **6. Building Enterprise Applications:**\n> \n> *   **CRM (Customer Relationship Management) Systems:** Managing customer interactions, sales, and marketing activities.\n> *   **ERP (Enterprise Resource Planning) Systems:** Integrating various business functions like finance, HR, and supply chain management.\n> *   **Supply Chain Management Systems:**  Optimizing the flow of goods and information from suppliers to customers.\n> \n> **In summary, system design is crucial for:**\n> \n> *   **Scalability:** Handling growing user base and data volume.\n> *   **Reliability:** Ensuring high availability and fault tolerance.\n> *   **Performance:** Optimizing response times and throughput.\n> *   **Security:** Protecting data and systems from threats.\n> *   **Maintainability:** Making the system easy to understand, modify, and debug.\n> *   **Cost-effectiveness:**  Designing a system that is efficient in terms of resource utilization and development costs.\n> \n> The specific system design considerations will vary depending on the specific requirements of the application.  However, a solid understanding of system design principles is essential for building successful and sustainable software systems."
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt -y -qq install tesseract-ocr libtesseract-dev\n",
        "\n",
        "!sudo apt-get -y -qq install poppler-utils libxml2-dev libxslt1-dev antiword unrtf poppler-utils pstotext tesseract-ocr flac ffmpeg lame libmad0 libsox-fmt-mp3 sox libjpeg-dev swig\n",
        "\n",
        "!pip install langchain\n",
        "\n",
        "!pip install -qU langchain_community pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-G-2mDEUVPfQ",
        "outputId": "4de9e782-dac3-45d1-d433-1b0d1a78104e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "libtesseract-dev is already the newest version (4.1.1-2.1build1).\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.59)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.42)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "import warnings\n",
        "from pathlib import Path as p\n",
        "from pprint import pprint\n",
        "\n",
        "import pandas as pd\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "QcDjuG38V2H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\",google_api_key=GEMINI_API_KEY,\n",
        "                             temperature=0.2,convert_system_message_to_human=True)"
      ],
      "metadata": {
        "id": "JfGypTtOWsh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Extract Text from PDF"
      ],
      "metadata": {
        "id": "SctKTuZJY3tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_loader = PyPDFLoader(\"/content/sample_data/MOE.pdf\")\n",
        "pages = pdf_loader.load_and_split()\n",
        "print(pages[3].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxciDTVyY65r",
        "outputId": "6d6b5917-a658-4f4a-ae0a-86356f2729e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Active\n",
            "Params MMLU HellaS WinoG PIQA Arc-e Arc-c NQ TriQA HumanE MBPP Math GSM8K\n",
            "LLaMA 2 7B 7B 44.4% 77.1% 69.5% 77.9% 68.7% 43.2% 17.5% 56.6% 11.6% 26.1% 3.9% 16.0%\n",
            "LLaMA 2 13B 13B 55.6% 80.7% 72.9% 80.8% 75.2% 48.8% 16.7% 64.0% 18.9% 35.4% 6.0% 34.3%\n",
            "LLaMA 1 33B 33B 56.8% 83.7% 76.2% 82.2% 79.6% 54.4% 24.1% 68.5% 25.0% 40.9% 8.4% 44.1%\n",
            "LLaMA 2 70B 70B 69.9% 85.4% 80.4% 82.6% 79.9% 56.5% 25.4% 73.0% 29.3% 49.8% 13.8% 69.6%\n",
            "Mistral 7B 7B 62.5% 81.0% 74.2% 82.2% 80.5% 54.9% 23.2% 62.5% 26.2% 50.2% 12.7% 50.0%\n",
            "Mixtral 8x7B 13B 70.6% 84.4% 77.2% 83.6% 83.1% 59.7% 30.6% 71.5% 40.2% 60.7% 28.4% 74.4%\n",
            "Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on\n",
            "almost all popular benchmarks while using 5x fewer active parameters during inference.\n",
            "Figure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension,\n",
            "math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B\n",
            "on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It\n",
            "is also vastly superior to Llama 2 70B on code and math.\n",
            "Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported\n",
            "in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different\n",
            "categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a\n",
            "superior performance in code and mathematics benchmarks.\n",
            "Size and Efficiency. We compare our performance to the Llama 2 family, aiming to understand\n",
            "Mixtral models’ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture-\n",
            "of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active\n",
            "parameters, Mixtral is able to outperform Llama 2 70B across most categories.\n",
            "Note that this analysis focuses on the active parameter count (see Section 2.1), which is directly\n",
            "proportional to the inference compute cost, but does not consider the memory costs and hardware\n",
            "utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count,\n",
            "47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer\n",
            "introduces additional overhead due to the routing mechanism and due to the increased memory loads\n",
            "when running more than one expert per device. They are more suitable for batched workloads where\n",
            "one can reach a good degree of arithmetic intensity.\n",
            "Comparison with Llama 2 70B and GPT-3.5.In Table 3, we report the performance of Mixtral 8x7B\n",
            "compared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the\n",
            "two other models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller\n",
            "capacity (47B tokens compared to 70B). For MT Bench, we report the performance of the latest\n",
            "GPT-3.5-Turbo model available,gpt-3.5-turbo-1106.\n",
            "2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RAG Pipeline"
      ],
      "metadata": {
        "id": "YIWLMOJqZOyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
      ],
      "metadata": {
        "id": "i-J2TykwZRgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
        "context = \"\\n\\n\".join(str(p.page_content) for p in pages)\n",
        "texts = text_splitter.split_text(context)"
      ],
      "metadata": {
        "id": "u4xZ2sXsZVxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=GEMINI_API_KEY)"
      ],
      "metadata": {
        "id": "u3QFdlhrZ4XK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_index = Chroma.from_texts(texts, embeddings).as_retriever(search_kwargs={\"k\":5})\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    model,\n",
        "    retriever=vector_index,\n",
        "    return_source_documents=True\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "1l_JF7pGZ7kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Describe what is written in the document?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "Markdown(result[\"result\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "mt1r8aPKaGaD",
        "outputId": "66fc3cc4-bcef-4953-d612-06ef21b4dbae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The document introduces Mixtral 8x7B, a sparse mixture of experts (SMoE) language model. It has the same architecture as Mistral 7B, but each layer has 8 feedforward blocks (experts). A router network selects two experts for each token at each layer. It was trained with a context size of 32k tokens and outperforms or matches Llama 2 70B and GPT-3.5 across evaluated benchmarks. A fine-tuned instruction model, Mixtral 8x7B – Instruct, surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B – chat on human benchmarks. Both models are released under the Apache 2.0 license. The document also details the model's architecture, performance on various benchmarks, multilingual capabilities, long-range performance, and bias benchmarks. It includes comparisons to other models like Llama 2 and GPT-3.5, and discusses the routing mechanism of the experts."
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)# Run chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    model,\n",
        "    retriever=vector_index,\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        ")"
      ],
      "metadata": {
        "id": "D_3rCCKRaRGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Describe what is written in the document?\"\n",
        "result = qa_chain({\"query\": question})"
      ],
      "metadata": {
        "id": "MmxzlBUVbc9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(result[\"result\"])"
      ],
      "metadata": {
        "id": "SQJS0FMoblhM",
        "outputId": "4f97d317-a893-4e92-9332-967da2d8754c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Mixtral 8x7B is a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. It outperforms Llama 2 70B and GPT-3.5 on most benchmarks. It has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens. A chat model fine-tuned to follow instructions, Mixtral 8x7B – Instruct, surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B – chat model on human benchmarks.\nthanks for asking!"
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}